{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmj-chosim/Commit-Project-2023.1.20-2023.2.28-/blob/main/2023.02.07/fmri%7C%7Ceeg/02_DNN_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XQBr4FtYvp-Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "import time\n",
        "import seaborn as sns\n",
        "import gc\n",
        "import os \n",
        "import random\n",
        "from datetime import datetime as dt\n",
        "from pytz import timezone\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6G-ybQJGvp-T",
        "outputId": "605c7eb5-1e97-4877-c5aa-130d25004dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount your Google drive to the Google colab notebook \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/My Drive/KNU_2023_FC_classification')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsPsH1H2vp-T"
      },
      "source": [
        "# Load input data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkFb6tZMvp-T",
        "outputId": "5ff97c8e-0727-4b71-bb40-f8e8ce351db5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-02-07 06:33:56--  https://bspl.korea.ac.kr/Research_data/KNU2023/FC_input.npz\n",
            "Resolving bspl.korea.ac.kr (bspl.korea.ac.kr)... 163.152.29.203\n",
            "Connecting to bspl.korea.ac.kr (bspl.korea.ac.kr)|163.152.29.203|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 509491930 (486M)\n",
            "Saving to: ‘FC_input.npz’\n",
            "\n",
            "FC_input.npz        100%[===================>] 485.89M  36.5MB/s    in 13s     \n",
            "\n",
            "2023-02-07 06:34:11 (36.0 MB/s) - ‘FC_input.npz’ saved [509491930/509491930]\n",
            "\n",
            "(3200, 19900) (3200, 1)\n"
          ]
        }
      ],
      "source": [
        "!wget 'https://bspl.korea.ac.kr/Research_data/KNU2023/FC_input.npz'\n",
        "fc_data = np.load('FC_input.npz')\n",
        "\n",
        "X = fc_data['x'] \n",
        "X = np.arctanh(X)\n",
        "X = stats.zscore(X, axis=1)\n",
        "y = fc_data['y'].astype(np.float_).copy().reshape(-1, 1)\n",
        "\n",
        "print(X.shape,y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gvhvWbPk2cO",
        "outputId": "8c4e3b0e-954b-4486-ac97-fe5cdd1f98a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_sbjs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wXcPpQFl7AG",
        "outputId": "8afdcaa5-8a09-4334-da4d-16d9581501da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 54,  11,  92,   4,  75,  69,   1,  63,  60,  42,  17,  66,  78,\n",
              "        20,  98,  43,  31,   8,  27,  39,  28,  65,  32,  62,  45,  96,\n",
              "        30,  29,  13,  41,  72,  35,  89,  71,  40,  56,  88,  25,  53,\n",
              "        46,   7,  44,  94,  16,  48,  82,  18,  68,  87,   2,  73,  76,\n",
              "        51,  47,  22,  59,  93,   3,  49,  50,  34,  36,  67,   5,  97,\n",
              "        24,  37,  74,  26,  23,  90,  77,  33,  83,  64,  91,  15,   9,\n",
              "        12,  70,  21,  38,  58,  84,  52,  86,  19,  10, 100,  55,  81,\n",
              "        79,  99,  57,  80,  61,  95,   6,  85,  14])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "VLBBrtX7vp-U"
      },
      "outputs": [],
      "source": [
        "sbjs = fc_data['sbjs']\n",
        "all_sbjs = np.unique(sbjs)\n",
        "np.random.shuffle(all_sbjs)\n",
        "\n",
        "train_sbjs = all_sbjs[0:80]\n",
        "val_sbjs = all_sbjs[80:90]\n",
        "test_sbjs = all_sbjs[90:100]\n",
        "\n",
        "train_sbjs_loc = np.in1d(sbjs,train_sbjs)\n",
        "valid_sbjs_loc = np.in1d(sbjs,val_sbjs)\n",
        "test_sbjs_loc = np.in1d(sbjs,test_sbjs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-pJDlTvvp-U"
      },
      "source": [
        "# Set hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "lNmM3Tzovp-V"
      },
      "outputs": [],
      "source": [
        "seed = 42\n",
        "\n",
        "learning_rate = 1e-5\n",
        "min_lr = 1e-10\n",
        "lr_patience = 10\n",
        "momentum = 0.90\n",
        "\n",
        "l2_param = 5e-5\n",
        "l1_param_cand = [5e-4,5e-5,5e-6]\n",
        "param_cand = {\"l1_param\": l1_param_cand}\n",
        "param_grid = list(ParameterGrid(param_cand))\n",
        "param_grid\n",
        "              \n",
        "h1 = 512\n",
        "h2 = 512\n",
        "\n",
        "dropout_h1 = 0.4\n",
        "dropout_h2 = 0.4\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 300\n",
        "print_epoch = 10\n",
        "\n",
        "input_dim = X.shape[1]\n",
        "output_dim = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L0zGORf2vp-V"
      },
      "source": [
        "# Setup Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "6SdiXvRzvp-W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau \n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYXSjSFgvp-W"
      },
      "source": [
        "### Set CPU/GPU usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "yV4nxaR4vp-W"
      },
      "outputs": [],
      "source": [
        "num_workers = 2\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"   \n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\") "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7-K-5t6vp-W"
      },
      "source": [
        "### Random seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pULntFR5vp-X"
      },
      "outputs": [],
      "source": [
        "def seed_everything(seed=seed):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKnj-jaqvp-X"
      },
      "source": [
        "# Define functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNnvaDbMvp-X"
      },
      "source": [
        "### Define functions for data load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Nwl2rYvcvp-X"
      },
      "outputs": [],
      "source": [
        "class GetDataset(Dataset): \n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    \n",
        "    def __getitem__(self, idx): \n",
        "        X = torch.from_numpy(self.X[idx]).type(torch.FloatTensor)\n",
        "        y = torch.from_numpy(self.y[idx]).type(torch.FloatTensor)\n",
        "\n",
        "        return X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtkkGI6jvp-Y"
      },
      "source": [
        "### Define functions for training/validaiton/test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "x9xPapvRvp-Y"
      },
      "outputs": [],
      "source": [
        "class DNN(nn.Module):\n",
        "    def __init__(self, h1, h2, dropout_h1, dropout_h2):\n",
        "        # 2 hidden layers\n",
        "        super(DNN, self).__init__()\n",
        "        self.ext_1 = nn.Linear(input_dim, h1)\n",
        "        self.ext_bn_1 = nn.BatchNorm1d(h1)\n",
        "        \n",
        "        self.prd_1 = nn.Linear(h1, h2)\n",
        "        self.prd_bn_1 = nn.BatchNorm1d(h2)\n",
        "        self.prd_2 = nn.Linear(h2, output_dim)\n",
        "        \n",
        "        self.dropout_h1 = nn.Dropout(p=dropout_h1)\n",
        "        self.dropout_h2 = nn.Dropout(p=dropout_h2)\n",
        "        \n",
        "        self.act_func = nn.Tanh()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.weights_init()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.ext_1(x)\n",
        "        x = self.ext_bn_1(x)\n",
        "        x = self.act_func(x)\n",
        "        x = self.dropout_h1(x)\n",
        "        \n",
        "        x = self.prd_1(x)\n",
        "        x = self.prd_bn_1(x)\n",
        "        x = self.act_func(x)\n",
        "        x = self.dropout_h2(x)\n",
        "        x = self.prd_2(x)         \n",
        "        out = self.sigmoid(x)\n",
        "        \n",
        "        return out\n",
        "    \n",
        "    def weights_init(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.kaiming_normal_(m.weight, mode=\"fan_in\", nonlinearity=\"relu\")\n",
        "                nn.init.normal_(m.bias, std=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Af_6mbIJvp-Y"
      },
      "outputs": [],
      "source": [
        "def train(model, epoch, train_loader, optimizer, criterion, l1_param, l2_param):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    train_acc = 0\n",
        "    cost = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for batch_idx, (input, target) in enumerate(train_loader):\n",
        "        optimizer.zero_grad() \n",
        "        input, target = input.to(device), target.to(device)\n",
        "        pred, target = model(input), target.view(-1, 1)\n",
        "        \n",
        "        loss = criterion(pred, target)\n",
        "        all_linear1_params = torch.cat([x.view(-1) for x in model.ext_1.parameters()])\n",
        "        all_linear2_params = torch.cat([x.view(-1) for x in model.prd_1.parameters()])\n",
        "        l1_regularization = l1_param * (torch.norm(all_linear1_params, 1) + torch.norm(all_linear2_params, 1))\n",
        "        l2_regularization = l2_param * (torch.norm(all_linear1_params, 2) + torch.norm(all_linear2_params, 2))\n",
        "\n",
        "        cost = loss + l1_regularization + l2_regularization\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "        total += pred.size(0)\n",
        "                \n",
        "        crite = torch.FloatTensor([0.5])\n",
        "        crite = crite.to(device)\n",
        "        prediction = pred >= crite  # 예측값이 0.5를 넘으면 True로 간주\n",
        "        correct_prediction = prediction.float() == target # 실제값과 일치하는 경우만 True로 간주\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도 계산\n",
        "        correct += accuracy\n",
        "        \n",
        "    train_loss = train_loss / total\n",
        "    train_acc = correct / (batch_idx+1)\n",
        "\n",
        "    return train_loss, train_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QeGATetGvp-Y"
      },
      "outputs": [],
      "source": [
        "def valid(model, epoch, valid_loader, criterion):\n",
        "    model.eval()\n",
        "    valid_loss = 0\n",
        "    valid_acc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for input, target in valid_loader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            pred, target = model(input), target.view(-1, 1)\n",
        "            loss = criterion(pred, target)\n",
        "            valid_loss += loss.item()\n",
        "            total += pred.size(0)\n",
        "            \n",
        "            crite = torch.FloatTensor([0.5])\n",
        "            crite = crite.to(device)\n",
        "            prediction = pred >= crite # 예측값이 0.5를 넘으면 True로 간주\n",
        "            correct_prediction = prediction.float() == target # 실제값과 일치하는 경우만 True로 간주\n",
        "            accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도 계산\n",
        "            correct += accuracy\n",
        "\n",
        "    valid_acc = correct\n",
        "    \n",
        "    return valid_loss, valid_acc "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "cixRSEhKvp-Z"
      },
      "outputs": [],
      "source": [
        "def test(model, epoch, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_acc = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for input, target in test_loader:\n",
        "            input, target = input.to(device), target.to(device)\n",
        "            pred, target = model(input), target.view(-1, 1)\n",
        "            loss = criterion(pred, target)\n",
        "            test_loss += loss.item()\n",
        "            total += pred.size(0)\n",
        "            \n",
        "            crite = torch.FloatTensor([0.5])\n",
        "            crite = crite.to(device)\n",
        "            prediction = pred >= crite  # 예측값이 0.5를 넘으면 True로 간주\n",
        "            correct_prediction = prediction.float() == target # 실제값과 일치하는 경우만 True로 간주\n",
        "            accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
        "            correct += accuracy\n",
        "            \n",
        "    test_acc = correct\n",
        "\n",
        "    return test_loss, test_acc "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICKTenXVvp-Z"
      },
      "source": [
        "### Define function for model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "wRUSbLIOvp-Z"
      },
      "outputs": [],
      "source": [
        "def run_train(params, i_model=0, rst_path=None, val_flg=0, print_epoch=10):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    X_train, y_train = X[train_sbjs_loc,:], y[train_sbjs_loc]\n",
        "    X_valid, y_valid = X[valid_sbjs_loc,:], y[valid_sbjs_loc]\n",
        "    X_test, y_test = X[test_sbjs_loc,:], y[test_sbjs_loc]\n",
        "            \n",
        "    train_dataset = GetDataset(X_train, y_train)\n",
        "    valid_dataset = GetDataset(X_valid, y_valid)\n",
        "    test_dataset = GetDataset(X_test, y_test)\n",
        "    \n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=batch_size, pin_memory=True,\n",
        "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "    valid_loader = DataLoader(\n",
        "        valid_dataset, batch_size=len(y_valid), pin_memory=True,\n",
        "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset, batch_size=len(y_test), pin_memory=True,\n",
        "        shuffle=True, num_workers=num_workers, drop_last=True)\n",
        "        \n",
        "    # Assign model \n",
        "    model = DNN(h1, h2, dropout_h1, dropout_h2).to(device)\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, nesterov=True)\n",
        "    scheduler = ReduceLROnPlateau(\n",
        "        optimizer, mode=\"min\", patience=lr_patience, min_lr=min_lr\n",
        "    )\n",
        "    criterion = nn.functional.binary_cross_entropy\n",
        "              \n",
        "    # list to save learning parameters\n",
        "    train_loss_list = []\n",
        "    valid_loss_list = []\n",
        "    train_acc_list = []\n",
        "    valid_acc_list = []\n",
        "    test_acc_list = []\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss, train_acc = train(\n",
        "            model, epoch, train_loader, optimizer, criterion, params['l1_param'], l2_param\n",
        "        )\n",
        "        valid_loss, valid_acc = valid(model, epoch, valid_loader, criterion)\n",
        "        test_loss, test_acc = test(model, epoch, test_loader, criterion)\n",
        "\n",
        "        scheduler.step(valid_loss)\n",
        "        lr = optimizer.param_groups[0]['lr']\n",
        "        \n",
        "        train_loss_list.append(train_loss)\n",
        "        train_acc_list.append(train_acc)\n",
        "        valid_loss_list.append(valid_loss)\n",
        "        valid_acc_list.append(valid_acc)\n",
        "        \n",
        "        if epoch % print_epoch == 0:\n",
        "            print(\"Epoch [{:d}/{:d}]\".format(epoch, epochs), end=\" \")\n",
        "            print(\"Lr: {:.1e}, Train loss: {:.4f}, Valid loss: {:.4f}, Train acc: {:.2f}, Valid acc: {:.2f}\"\n",
        "                  .format(lr, train_loss, valid_loss, train_acc, valid_acc))\n",
        "\n",
        "            plot_learning_curves(\n",
        "                rst_path, epochs, train_loss_list, valid_loss_list,  \n",
        "                train_acc_list, valid_acc_list, i_model, val_flg=1\n",
        "            )\n",
        "    \n",
        "    torch.save(model.state_dict(), \n",
        "               rst_path + \"/model_\" + str(i_model+1) + \".pt\")\n",
        "    \n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    \n",
        "    acc_list = [train_acc, valid_acc, test_acc]\n",
        "\n",
        "    tot_time = time.time() - start_time\n",
        "    print(\"\\nExecution Time for model: {:.2f} mins\".format(tot_time / 60))\n",
        "    \n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tof-fVsJvp-Z"
      },
      "source": [
        "### Define functions for saving results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "M1gNC46rvp-a"
      },
      "outputs": [],
      "source": [
        "def plot_learning_curves(\n",
        "    rst_path, epochs, train_loss, valid_loss, train_acc, valid_acc, i_model=0, val_flg=0):\n",
        "    \n",
        "    sns.set(style=\"darkgrid\", font_scale=2)\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(18, 10))\n",
        "    ax = ax.flat\n",
        "    lw = 4\n",
        "    last_epoch = epochs\n",
        "    \n",
        "    ax[0].plot(train_loss[:last_epoch], label='train loss', lw=lw, color=\"r\")\n",
        "    if val_flg:\n",
        "        ax[0].plot(valid_loss[:last_epoch], label='valid loss', lw=lw, color=\"g\")\n",
        "    ax[0].set_yscale('log')\n",
        "    ax[0].legend()\n",
        "    ax[0].set_title(\"Loss Plot\", pad=10)\n",
        "\n",
        "    ax[1].plot(train_acc[:last_epoch], label='train acc', lw=lw, color=\"r\")\n",
        "    if val_flg:\n",
        "        ax[1].plot(valid_acc[:last_epoch], label='valid acc', lw=lw, color=\"g\")\n",
        "    ax[1].legend()\n",
        "    ax[1].set_title(\"Accuracy Plot\", pad=10)\n",
        "    \n",
        "    fig.tight_layout()\n",
        "    fig.savefig(\"{}/Learning_curves_model_{}.png\".format(rst_path,i_model+1))\n",
        "    \n",
        "    plt.close(fig)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXOcxdCyvp-a"
      },
      "source": [
        "# Create directory & save model info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FlSecUV5vp-a",
        "outputId": "522dcb4e-337b-47fd-a674-117b9d113c7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result_20230207_155657\n"
          ]
        }
      ],
      "source": [
        "nowtime = dt.now(timezone(\"Asia/Seoul\"))\n",
        "\n",
        "year = str(nowtime.year)\n",
        "month = '0{}'.format(nowtime.month) if nowtime.month < 10 else str(nowtime.month)\n",
        "day = '0{}'.format(nowtime.day) if nowtime.day < 10 else str(nowtime.day)\n",
        "hour = '0{}'.format(nowtime.hour) if nowtime.hour < 10 else str(nowtime.hour)\n",
        "minute = '0{}'.format(nowtime.minute) if nowtime.minute < 10 else str(nowtime.minute)\n",
        "sec = '0{}'.format(nowtime.second) if nowtime.second < 10 else str(nowtime.second)\n",
        "\n",
        "save_path = \"result_{}{}{}_{}{}{}\".format(year,month,day,hour,minute,sec)\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "rst_path = \"{}/train_rst\".format(save_path)\n",
        "os.makedirs(rst_path, exist_ok=True)\n",
        "\n",
        "\n",
        "f = open(save_path+\"/info.txt\",'w')\n",
        "f.write('seed : '+str(seed)+'\\n')\n",
        "f.write('learning_rate : '+str(learning_rate)+'\\n')\n",
        "f.write('min_lr : '+str(min_lr)+'\\n')\n",
        "f.write('lr_patience : '+str(lr_patience)+'\\n')\n",
        "f.write('momentum : '+str(momentum)+'\\n')\n",
        "f.write('l2_param : '+str(l2_param)+'\\n')\n",
        "f.write('dropout_h1 : '+str(dropout_h1)+'\\n')\n",
        "f.write('dropout_h2 : '+str(dropout_h2)+'\\n')\n",
        "f.write('batch_size : '+str(batch_size)+'\\n')\n",
        "f.write('input_dim : '+str(input_dim)+'\\n')\n",
        "f.write('output_dim : '+str(output_dim)+'\\n')\n",
        "f.write('epochs : '+str(epochs)+'\\n')\n",
        "f.close()\n",
        "np.savez(save_path+'/sbj_shuffle',train_sbjs=train_sbjs,val_sbjs=val_sbjs,test_sbjs=test_sbjs)\n",
        "\n",
        "print(save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TR__0E2Ovp-a"
      },
      "source": [
        "# Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TOjXmRskvp-a",
        "outputId": "fb61be0a-2588-46c6-8453-59ed38ac804d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "<1> {'l1_param': 0.0005}\n",
            "Epoch [10/300] Lr: 1.0e-05, Train loss: 0.0054, Valid loss: 0.6180, Train acc: 0.61, Valid acc: 0.68\n",
            "Epoch [20/300] Lr: 1.0e-05, Train loss: 0.0045, Valid loss: 0.5423, Train acc: 0.70, Valid acc: 0.75\n",
            "Epoch [30/300] Lr: 1.0e-05, Train loss: 0.0041, Valid loss: 0.4944, Train acc: 0.74, Valid acc: 0.77\n"
          ]
        }
      ],
      "source": [
        "all_acc_list = []\n",
        "for i_model,params in enumerate(param_grid):\n",
        "\n",
        "    print(\"\\n<{}> {}\".format(i_model+1,params))\n",
        "    train_acc_model, valid_acc_model, test_acc_model = run_train(\n",
        "        params, i_model, rst_path, val_flg = 0, print_epoch=print_epoch\n",
        "    )\n",
        "    all_acc_list.append([params, train_acc_model, valid_acc_model, test_acc_model])\n",
        "\n",
        "        \n",
        "acc_df = pd.DataFrame(np.array(all_acc_list), columns=[\"params\",\"train_acc\", \"valid_acc\", \"test_acc\"])\n",
        "acc_df.to_csv(\"{}/params_acc.csv\".format(rst_path))\n",
        "\n",
        "all_valid_acc = [item[2] for item in all_acc_list]\n",
        "selected_model = np.argmax(all_valid_acc)\n",
        "\n",
        "print(\"\\n<{}> Selected {}, train acc: {:.4f}, valid acc: {:.4f}, test acc: {:.4f}\"\n",
        "      .format(selected_model+1, param_grid[selected_model], all_acc_list[selected_model][1], all_acc_list[selected_model][2], all_acc_list[selected_model][3]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}