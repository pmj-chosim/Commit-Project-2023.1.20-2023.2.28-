{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxpts4ja+qJhPlIR8rZZUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmj-chosim/Commit-Project-2023.1.20-2023.2.28-/blob/main/2023.02.13/nlp/transformerchatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Transformer Chatbot"
      ],
      "metadata": {
        "id": "tIoZQ-f8LFkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHntFscwK_ZZ",
        "outputId": "26af0762-24f0-4e06-af0c-04766d0ae659"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.97\n"
          ]
        }
      ],
      "source": [
        "! pip install sentencepiece "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch \n",
        "import sentencepiece as spm\n",
        "import sys\n",
        "from tqdm import tqdm\n",
        "from torch.nn import Transformer\n",
        "from torch import nn\n",
        "import math\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "RX3oN9c1LEut"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter\n",
        "MAX_LENGTH = 40\n",
        "BATCH_SIZE = 64\n",
        "lr = 1e-4\n",
        "embed_size = 256 \n",
        "n_head=8\n",
        "n_hid = 512\n",
        "n_layer = 2\n",
        "dropout = 0.1\n",
        "epoch = 30\n",
        "\n",
        "#seed\n",
        "random_seed = 9712\n",
        "torch.manual_seed(random_seed)\n",
        "torch.backends.cudnn.enabled = False\n",
        "print(torch.randn(1, 3))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8jn-jLeLEw4",
        "outputId": "544c5da1-28f2-48ff-958c-736b3d9e17d9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.3936, 0.5584, 0.9692]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#GPU\n",
        "! nvidia-smi\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SB__yVrCLUMw",
        "outputId": "8f4f935b-0c78-41eb-81e5-9f4cc798b3c7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Feb 13 05:09:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0    28W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data \n",
        "train_data = pd.read_csv('https://raw.githubusercontent.com/Doheon/Chatbot-Transformer/main/ChatBotData.csv')\n",
        "\n",
        "questions = []\n",
        "for sentence in train_data['Q']:\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    questions.append(sentence)\n",
        "\n",
        "\n",
        "answers = []\n",
        "for sentence in train_data['A']:\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    answers.append(sentence)\n",
        "\n",
        "with open('all.txt', 'w', encoding='utf8') as f:\n",
        "    f.write('\\n'.join(questions))\n",
        "    f.write('\\n'.join(answers))\n",
        "\n",
        "\n",
        "corpus = \"all.txt\"\n",
        "prefix = \"chatbot\"\n",
        "vocab_size = 16000\n",
        "spm.SentencePieceTrainer.train(\n",
        "    f\"--input={corpus} --model_prefix={prefix} --vocab_size={vocab_size + 7}\" + \n",
        "    \"--min_frequency={3}\"+\n",
        "    \" --model_type=bpe\" +\n",
        "    \" --max_sentence_length=999999\" + # 문장 최대 길이\n",
        "    \" --pad_id=0 --pad_piece=[PAD]\" + # pad (0)\n",
        "    \" --unk_id=1 --unk_piece=[UNK]\" + # unknown (1)\n",
        "    \" --bos_id=2 --bos_piece=[BOS]\" + # begin of sequence (2)\n",
        "    \" --eos_id=3 --eos_piece=[EOS]\" + # end of sequence (3)\n",
        "    \" --user_defined_symbols=[SEP],[CLS],[MASK]\") # 사용자 정의 토큰\n",
        "\n",
        "vocab_file = \"chatbot.model\"\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.load(vocab_file)\n",
        "line = \"안녕하세요 만나서 반갑습니다\"\n",
        "pieces = vocab.encode_as_pieces(line)\n",
        "ids = vocab.encode_as_ids(line)\n",
        "\n",
        "\n",
        "print(line)\n",
        "print(pieces)\n",
        "print(ids)\n",
        "print(vocab.GetPieceSize())\n",
        "vocab_size = vocab.GetPieceSize()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qx2q9zgLUPP",
        "outputId": "0f5e5381-022a-4672-dab6-485165cae5a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요 만나서 반갑습니다\n",
            "['▁안녕하세요', '▁만나서', '▁반갑습니다']\n",
            "[4626, 1930, 8499]\n",
            "16007\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "START_TOKEN = [2]\n",
        "END_TOKEN = [3]\n",
        "\n",
        "#tokenize and padding  \n",
        "\n",
        "def tokenize_and_filter(inputs, outputs):\n",
        "  tokenized_inputs, tokenized_outputs = [], []\n",
        "  for (sentence1, sentence2) in zip(inputs, outputs):\n",
        "    zeros1 = np.zeros(MAX_LENGTH, dtype=int)\n",
        "    zeros2 = np.zeros(MAX_LENGTH, dtype=int)\n",
        "    sentence1 = START_TOKEN + vocab.encode_as_ids(sentence1) + END_TOKEN\n",
        "    zeros1[:len(sentence1)] = sentence1[:MAX_LENGTH]\n",
        "\n",
        "    sentence2 = START_TOKEN + vocab.encode_as_ids(sentence2) + END_TOKEN\n",
        "    zeros2[:len(sentence2)] = sentence2[:MAX_LENGTH]\n",
        "\n",
        "    tokenized_inputs.append(zeros1)\n",
        "    tokenized_outputs.append(zeros2)\n",
        "  return tokenized_inputs, tokenized_outputs\n",
        "\n",
        "questions_encode, answers_encode = tokenize_and_filter(questions, answers)\n",
        "print(questions_encode[0])\n",
        "print(answers_encode[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6D6xZsBLEzQ",
        "outputId": "95187ea2-24f1-4029-f7e9-4f25a356bd0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[    2  5566 14968  3210   111     3     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0]\n",
            "[   2 5192  217 5936    7    3    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self, questions, answers):\n",
        "        questions = np.array(questions)\n",
        "        answers = np.array(answers)\n",
        "        self.inputs = questions\n",
        "        self.dec_inputs = answers[:,:-1] #(datanum, max_len-1)\n",
        "        self.outputs = answers[:,1:] #(datanum, max_len-1) \n",
        "        self.length = len(questions) #input_length \n",
        "    \n",
        "    def __getitem__(self,idx):\n",
        "        return (self.inputs[idx], self.dec_inputs[idx], self.outputs[idx])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "dataset = MyDataset(questions_encode, answers_encode)\n",
        "dataloader = DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "print(f\"data set num: {len(dataset)}\")\n",
        "print(f\"data set num: {len(dataloader)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3c8O5jyLE1Y",
        "outputId": "2e8810da-8231-400d-ca0d-d057d7d601a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data set num: 11823\n",
            "data set num: 185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, n_head, n_hid, n_layer, dropout=0.5):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.transformer = Transformer(embed_size, n_head, dim_feedforward=n_hid, num_encoder_layers=n_layer, num_decoder_layers=n_layer,dropout=dropout)\n",
        "        self.e_pos = PositionalEncoding(embed_size, dropout)\n",
        "        self.e_embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.d_pos = PositionalEncoding(embed_size, dropout)\n",
        "        self.encoder_d = nn.Embedding(vocab_size, embed_size)\n",
        "        self.embed_size = embed_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.linear = nn.Linear(embed_size, vocab_size)\n",
        "        self.init_weights()\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.e_embedding.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, src, tgt, srcmask, tgtmask, srcpadmask, tgtpadmask):\n",
        "        src = self.e_embedding(src) * math.sqrt(self.embed_size) #(batch_size, max_len, embed_size)\n",
        "        src = self.e_pos(src) #(batch_size, max_len, embed_size)\n",
        "        tgt = self.encoder_d(tgt) * math.sqrt(self.embed_size)#(batch_size, max_len-1, embed_size)\n",
        "        tgt = self.d_pos(tgt)#(batch_size, max_len-1, embed_size)\n",
        "        output = self.transformer(src.transpose(0,1), tgt.transpose(0,1), srcmask, tgtmask, src_key_padding_mask=srcpadmask, tgt_key_padding_mask=tgtpadmask) #(max_len-1, batch_size,embed_size)\n",
        "        output = self.linear(output) #???\n",
        "        return output\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "def gen_attention_mask(x):\n",
        "    mask = torch.eq(x, 0)\n",
        "    return mask\n",
        "    \n",
        "model = TransformerModel(vocab_size, embed_size=embed_size, n_head=n_head, n_hid=n_hid, n_layer=n_layer, dropout=dropout).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "popWkETSLE34"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "for i in range(epoch):\n",
        "    batchloss = 0.0\n",
        "    progress = tqdm(dataloader)\n",
        "    for (inputs, dec_inputs, outputs) in progress:\n",
        "        optimizer.zero_grad()\n",
        "        src_mask = model.generate_square_subsequent_mask(MAX_LENGTH).to(device) #(max_len, max_len)\n",
        "        src_padding_mask = gen_attention_mask(inputs).to(device) #(batch_size, max_len)\n",
        "        tgt_mask = model.generate_square_subsequent_mask(MAX_LENGTH-1).to(device)  #(max_len-1, max_len-1)\n",
        "        tgt_padding_mask = gen_attention_mask(dec_inputs).to(device) #(batch_size, max_len-1)\n",
        "        result = model(inputs.to(device), dec_inputs.to(device), src_mask, tgt_mask, src_padding_mask,tgt_padding_mask) #(max_len, batch_size, vocab_size)\n",
        "        loss = criterion(result.permute(1,2,0), outputs.to(device).long())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batchloss += loss\n",
        "        progress.set_description(\"{:0.3f}\".format(loss))\n",
        "    print(\"epoch:\",i+1,\"|\",\"loss:\",batchloss.cpu().item() / len(dataloader))\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv2ByGLJL48x",
        "outputId": "996116a7-9ce4-4b9f-f592-05e6f7b4ae27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1.084: 100%|██████████| 185/185 [00:16<00:00, 11.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 | loss: 1.8357174435177366\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.909: 100%|██████████| 185/185 [00:12<00:00, 14.30it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2 | loss: 0.9696116679423564\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.770: 100%|██████████| 185/185 [00:12<00:00, 14.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 3 | loss: 0.9100667592641469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.964: 100%|██████████| 185/185 [00:12<00:00, 14.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 4 | loss: 0.8817562618771115\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.874: 100%|██████████| 185/185 [00:12<00:00, 14.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 5 | loss: 0.859440159153294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.776: 100%|██████████| 185/185 [00:13<00:00, 14.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 6 | loss: 0.8401062836518158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.927: 100%|██████████| 185/185 [00:13<00:00, 13.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 7 | loss: 0.8214277937605574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.806: 100%|██████████| 185/185 [00:13<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 8 | loss: 0.8023002830711571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.677: 100%|██████████| 185/185 [00:13<00:00, 14.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 9 | loss: 0.7826237034153294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.660: 100%|██████████| 185/185 [00:12<00:00, 14.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10 | loss: 0.7623109559755068\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.678: 100%|██████████| 185/185 [00:12<00:00, 14.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 11 | loss: 0.7418268976984798\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.694: 100%|██████████| 185/185 [00:13<00:00, 14.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 12 | loss: 0.7208734460779138\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.677: 100%|██████████| 185/185 [00:13<00:00, 14.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 13 | loss: 0.7003096296980574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.669: 100%|██████████| 185/185 [00:13<00:00, 13.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 14 | loss: 0.6794185329127956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.650: 100%|██████████| 185/185 [00:12<00:00, 14.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 15 | loss: 0.6584393475506757\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.670: 100%|██████████| 185/185 [00:12<00:00, 14.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 16 | loss: 0.6366320223421664\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.592: 100%|██████████| 185/185 [00:12<00:00, 14.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 17 | loss: 0.6154388427734375\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.599: 100%|██████████| 185/185 [00:12<00:00, 14.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 18 | loss: 0.592549298260663\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.583: 100%|██████████| 185/185 [00:12<00:00, 14.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 19 | loss: 0.5707731917097761\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.537: 100%|██████████| 185/185 [00:12<00:00, 14.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 20 | loss: 0.5485796954180743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.520: 100%|██████████| 185/185 [00:12<00:00, 14.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 21 | loss: 0.5265193217509502\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.539: 100%|██████████| 185/185 [00:12<00:00, 14.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 22 | loss: 0.5038204708614865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.391: 100%|██████████| 185/185 [00:12<00:00, 14.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 23 | loss: 0.48164132607949744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.481: 100%|██████████| 185/185 [00:13<00:00, 14.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 24 | loss: 0.45919478132918073\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.389: 100%|██████████| 185/185 [00:12<00:00, 14.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 25 | loss: 0.4379471649994721\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.403: 100%|██████████| 185/185 [00:13<00:00, 14.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 26 | loss: 0.4165530642947635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.427: 100%|██████████| 185/185 [00:12<00:00, 14.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 27 | loss: 0.3945894808382601\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.301: 100%|██████████| 185/185 [00:13<00:00, 14.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 28 | loss: 0.373946153795397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.416: 100%|██████████| 185/185 [00:12<00:00, 14.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 29 | loss: 0.35336580018739444\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "0.395: 100%|██████████| 185/185 [00:12<00:00, 14.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 30 | loss: 0.33408702128642315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"chatbot.pth\")"
      ],
      "metadata": {
        "id": "Ro5m20gwL5E4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_sentence(sentence):\n",
        "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
        "    sentence = sentence.strip()\n",
        "    return sentence\n",
        "\n",
        "def evaluate(sentence):\n",
        "    sentence = preprocess_sentence(sentence)\n",
        "    input = torch.tensor([START_TOKEN + vocab.encode_as_ids(sentence) + END_TOKEN]).to(device)\n",
        "    output = torch.tensor([START_TOKEN]).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    for i in range(MAX_LENGTH):\n",
        "        #mask \n",
        "        src_mask = model.generate_square_subsequent_mask(input.shape[1]).to(device)\n",
        "        tgt_mask = model.generate_square_subsequent_mask(output.shape[1]).to(device)\n",
        "        src_padding_mask = gen_attention_mask(input).to(device)\n",
        "        tgt_padding_mask = gen_attention_mask(output).to(device)\n",
        "\n",
        "        predictions = model(input, output, src_mask, tgt_mask, src_padding_mask, tgt_padding_mask).transpose(0,1) #(batch_size, output_size, vocab_size)\n",
        "\n",
        "        # 현재(마지막) 시점의 예측 단어를 받아온다.\n",
        "        predictions = predictions[:, -1:, :]\n",
        "        predicted_id = torch.LongTensor(torch.argmax(predictions.cpu(), axis=-1))\n",
        "\n",
        "\n",
        "        # 만약 마지막 시점의 예측 단어가 종료 토큰이라면 예측을 중단\n",
        "        if torch.equal(predicted_id[0][0], torch.tensor(END_TOKEN[0])):\n",
        "            break\n",
        "\n",
        "        # 마지막 시점의 예측 단어를 출력에 연결한다.\n",
        "        # 이는 for문을 통해서 디코더의 입력으로 사용될 예정이다.\n",
        "        output = torch.cat([output, predicted_id.to(device)], axis=1)\n",
        "\n",
        "    return torch.squeeze(output, axis=0).cpu().numpy()\n",
        "\n",
        "def predict(sentence):\n",
        "    prediction = evaluate(sentence)\n",
        "    predicted_sentence = vocab.Decode(list(map(int,[i for i in prediction if i < vocab_size])))\n",
        "    \n",
        "    print(\"========================================\")\n",
        "    print('Q: {}'.format(sentence))\n",
        "    print('A: {}'.format(predicted_sentence))"
      ],
      "metadata": {
        "id": "_qXr7mrJNwmx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"chatbot.pth\"))\n",
        "result = predict(\"게임하고 싶어\")\n",
        "result = predict(\"놀고싶다\")\n",
        "result = predict(\"감기 같애\")\n",
        "result = predict(\"건강하게 다이어트 하는 방법\")\n",
        "result=predict(\"안녕\")\n",
        "result = predict(\"궁금하지?\")\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhwY64dBLE6a",
        "outputId": "882e42f1-4dbd-4ba7-be95-7850fe6237fd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================================\n",
            "Q: 게임하고 싶어\n",
            "A: 새로운 스타일 도전해 보시면 어때요 !\n",
            "========================================\n",
            "Q: 놀고싶다\n",
            "A: 직장 스트레스가 심한가봐요 마세요 .\n",
            "========================================\n",
            "Q: 감기 같애\n",
            "A: 병원가세요 게 좋겠죠 .\n",
            "========================================\n",
            "Q: 건강하게 다이어트 하는 방법\n",
            "A: 너무 자책하지 마세요 .\n",
            "========================================\n",
            "Q: 안녕\n",
            "A: 후회해도 늦었어요 하네요 .\n",
            "========================================\n",
            "Q: 궁금하지?\n",
            "A: 아직 사귀는 경우도 있지만 정확하게 아직 모르겠어요 .\n"
          ]
        }
      ]
    }
  ]
}